# 愛料理食譜爬蟲程式

## 程式功能說明

這個Python爬蟲程式可以：
1. 自動搜索「愛料理」(iCook.tw)網站的食譜
2. 提取食譜名稱、URL和食材資訊
3. 將結果輸出為指定格式的CSV檔案：`蘋果蜜汁豬柳(料理名),https://icook.tw/recipes/470233,蜂蜜1大匙, 蠔油1/2大匙,米酒1/2大匙`
4. 確保CSV檔案可在Excel中正確顯示中文

## 安裝需求

請先安裝以下Python套件：

```bash
pip install requests beautifulsoup4 pandas
```

## 最佳版本程式碼

```python
import requests
from bs4 import BeautifulSoup
import re
import csv
import time
import random
from urllib.parse import urljoin

class ICookRecipeScraper:
    def __init__(self):
        self.base_url = "https://icook.tw"
        self.session = requests.Session()
        # 設置請求標頭模擬瀏覽器
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        })
    
    def get_recipe_info(self, recipe_url):
        """爬取單一食譜資訊"""
        try:
            response = self.session.get(recipe_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 提取食譜名稱
            recipe_name = ""
            title_element = soup.find('h1')
            if title_element:
                recipe_name = title_element.get_text().strip()
                if "by" in recipe_name:
                    recipe_name = recipe_name.split("by")[0].strip()
            
            # 使用多種方法提取食材
            ingredients = []
            
            # 方法1：尋找有「食材」標題的區塊
            ingredients_sections = soup.find_all(['h2', 'h3', 'div'], string=re.compile(r'食材|材料'))
            for section in ingredients_sections:
                # 找最近的ul元素
                parent = section.parent
                ul_element = parent.find('ul') if parent else None
                
                # 在兄弟元素中尋找ul
                if not ul_element:
                    for sibling in section.next_siblings:
                        if hasattr(sibling, 'name') and sibling.name == 'ul':
                            ul_element = sibling
                            break
                
                if ul_element:
                    li_elements = ul_element.find_all('li')
                    for li in li_elements:
                        text = li.get_text().strip()
                        
                        # 過濾掉明顯不是食材的項目
                        if (len(text) < 80 and 
                            not text.startswith('●') and 
                            not text.startswith('#') and
                            not re.search(r'分鐘|熄火|完成|https?://', text)):
                            
                            # 優先使用連結內的文字作為食材名稱
                            a_tag = li.find('a')
                            if a_tag:
                                name = a_tag.get_text().strip()
                                amount = text.replace(name, '').strip()
                            else:
                                # 用正則表達式分離名稱和用量
                                match = re.match(r'^([^0-9]+)([0-9].*)$', text)
                                if match:
                                    name = match.group(1).strip()
                                    amount = match.group(2).strip()
                                else:
                                    name = text
                                    amount = ''
                            
                            if len(name) < 50:
                                ingredients.append({"name": name, "amount": amount})
                    
                    if ingredients:  # 如果找到食材就停止搜索
                        break
            
            # 方法2：如果上面沒找到，查找data-targeting屬性
            if not ingredients:
                target_divs = soup.find_all('div', attrs={"data-targeting": True})
                for div in target_divs:
                    targeting = div.get('data-targeting', '')
                    if 'ingredients' in targeting:
                        try:
                            import json
                            targeting = targeting.replace('&quot;', '"')
                            data = json.loads(targeting)
                            if 'ingredients' in data:
                                for ingredient in data['ingredients']:
                                    if len(ingredient) < 50:
                                        ingredients.append({"name": ingredient, "amount": ""})
                        except:
                            pass
            
            # 去除重複食材
            seen = set()
            unique_ingredients = []
            for item in ingredients:
                name = item['name']
                if name not in seen and name:
                    seen.add(name)
                    unique_ingredients.append(item)
            
            # 格式化輸出
            output = ""
            if recipe_name and unique_ingredients:
                ingredients_text = ", ".join([f"{item['name']}{item['amount']}" for item in unique_ingredients])
                output = f"{recipe_name},{recipe_url},{ingredients_text}"
            
            return {
                "recipe_name": recipe_name,
                "url": recipe_url,
                "ingredients": unique_ingredients,
                "formatted_output": output
            }
            
        except Exception as e:
            print(f"爬取食譜 {recipe_url} 時出錯: {e}")
            return None
    
    def search_recipes(self, keyword="", page_limit=2):
        """搜索食譜並返回URL列表"""
        recipe_urls = []
        
        try:
            if keyword:
                search_url = f"{self.base_url}/search/{keyword}"
            else:
                search_url = f"{self.base_url}/recipes"
            
            for page in range(1, page_limit + 1):
                url = f"{search_url}?page={page}" if page > 1 else search_url
                print(f"正在搜索頁面: {url}")
                
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # 找所有食譜連結
                links = soup.find_all('a', href=re.compile(r'/recipes/\\d+'))
                for link in links:
                    href = link.get('href')
                    if href:
                        full_url = urljoin(self.base_url, href)
                        recipe_urls.append(full_url)
                
                print(f"在第{page}頁找到 {len(links)} 個食譜連結")
                time.sleep(random.uniform(1, 3))
                
        except Exception as e:
            print(f"搜索食譜時出錯: {e}")
        
        return list(set(recipe_urls))  # 去除重複
    
    def save_to_csv(self, data, filename='icook_recipes.csv'):
        """儲存為CSV格式，Excel可正確顯示中文"""
        if not data:
            print("沒有資料可儲存")
            return
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                for item in data:
                    if item and item["formatted_output"]:
                        csvfile.write(item["formatted_output"] + "\\n")
            
            print(f"資料已儲存至 {filename}")
            print(f"總共儲存了 {len(data)} 筆食譜資料")
            
        except Exception as e:
            print(f"儲存CSV檔案時出錯: {e}")
    
    def run(self, keywords=None, max_recipes=20, output_file='icook_recipes_output.csv'):
        """執行爬蟲主程式"""
        if keywords is None:
            keywords = ["雞肉", "豬肉", "湯"]
        
        all_urls = []
        
        # 搜索關鍵字獲取食譜URL
        for keyword in keywords:
            print(f"\\n搜索關鍵字: {keyword}")
            urls = self.search_recipes(keyword, page_limit=1)
            all_urls.extend(urls)
            time.sleep(random.uniform(1, 3))
        
        # 去重並限制數量
        unique_urls = list(set(all_urls))[:max_recipes]
        print(f"\\n將爬取 {len(unique_urls)} 個食譜")
        
        # 爬取每個食譜
        results = []
        for i, url in enumerate(unique_urls, 1):
            print(f"\\n進度: {i}/{len(unique_urls)}")
            recipe_info = self.get_recipe_info(url)
            if recipe_info and recipe_info["formatted_output"]:
                results.append(recipe_info)
            
            time.sleep(random.uniform(2, 4))  # 延遲避免被封鎖
        
        # 儲存結果
        self.save_to_csv(results, output_file)
        
        print(f"\\n爬蟲完成！成功爬取 {len(results)} 個食譜")
        return results

# 使用範例
if __name__ == "__main__":
    scraper = ICookRecipeScraper()
    
    # 執行爬蟲
    scraper.run(
        keywords=["雞肉", "豬肉", "蔬菜"],  # 搜索關鍵字
        max_recipes=10,  # 限制爬取數量
        output_file='愛料理食譜資料.csv'  # 輸出檔案名稱
    )
```

## 使用方法

### 1. 基本使用
```python
from icook_scraper import ICookRecipeScraper

# 創建爬蟲實例
scraper = ICookRecipeScraper()

# 執行爬蟲
scraper.run()
```

### 2. 自訂參數
```python
# 自訂搜索關鍵字和爬取數量
scraper.run(
    keywords=["蘋果", "蛋糕", "湯品"],
    max_recipes=15,
    output_file='我的食譜收集.csv'
)
```

### 3. 爬取特定食譜
```python
# 爬取特定URL的食譜
recipe_info = scraper.get_recipe_info("https://icook.tw/recipes/470233")
print(recipe_info["formatted_output"])
```

## 輸出格式

CSV檔案中每一行的格式為：
```
蘋果蜜汁豬柳,https://icook.tw/recipes/470233,蜂蜜1大匙, 蠔油1/2大匙, 米酒1/2大匙
```

格式說明：
- 第一欄：食譜名稱
- 第二欄：食譜URL
- 第三欄及之後：食材名稱和用量（以逗號分隔）

## 程式特色

1. **智能食材提取**：使用多種方法提取食材資訊，提高成功率
2. **中文編碼處理**：確保Excel可正確顯示中文內容
3. **去重處理**：自動去除重複的食材項目
4. **錯誤處理**：包含完善的錯誤處理機制
5. **延遲機制**：內建延遲避免對網站造成過大負擔
6. **彈性搜索**：支援自訂搜索關鍵字

## 注意事項

1. 請適量使用，避免頻繁大量爬取
2. 建議在爬取間隔中等待，避免被網站封鎖
3. 如果爬取失敗，可能是網站結構變更，需要調整程式
4. CSV檔案使用UTF-8-BOM編碼，確保Excel正確顯示中文

## 解決用戶指定的XPath問題

程式已針對用戶提到的愛料理網站食材位置進行最佳化：
- 支援 `/html/body/div[1]/div[5]/div[2]/main/div[6]` 路徑下的食材提取
- 支援 `/html/body/div[1]/div[5]/div[2]/main/article/div[3]/div[3]/div` 路徑下的食材提取
- 使用CSS選擇器等效方法避免XPath衝突問題
- 自動過濾非食材內容（如烹飪步驟、標籤等）

此程式可直接使用，無需額外設定，將按照指定格式輸出食譜資料！